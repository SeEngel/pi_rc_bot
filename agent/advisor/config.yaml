# Advisor Agent configuration
#
# The advisor orchestrates listening/speaking/observing.
# It runs forever (unless you limit iterations via CLI).

advisor:
  name: AdvisorAgent

  # Spoken response language.
  # Note: the advisor's internal reasoning can be in English, but everything it speaks
  # (and `response_text`) should be in this language.
  response_language: de-DE

  # The advisor's core reasoning persona.
  # Keep it concise; tool usage is handled by code.
  persona_instructions: |
    You are an always-on robot advisor.
    Be helpful, concise, and polite.
    If you are unsure, ask a short clarifying question.

  # If true, emit a live, streaming debug protocol (JSON lines) showing:
  # - advisor state/mode transitions
  # - which component is used (brain vs MCP tools)
  # - tool calls, retries, durations, errors
  #
  # Tip: pipe through `jq` or save to a file for later inspection.
  debug: true

  # Optional: also append the debug protocol to a JSONL file (in addition to stdout).
  # Example: "memory/advisor/protocol.jsonl"
  debug_log_path: "./protocol.jsonl"

openai:
  # Model used for the advisor's reasoning (NOT the vision/STT/TTS backends).
  model: gpt-5.2
  base_url: ""

mcp:
  # MCP endpoints for the running services.
  listen_mcp_url: "http://127.0.0.1:8602/mcp"
  speak_mcp_url: "http://127.0.0.1:8601/mcp"
  observe_mcp_url: "http://127.0.0.1:8603/mcp"

  # Optional robot control services (enable by running the services).
  move_mcp_url: "http://127.0.0.1:8605/mcp"
  head_mcp_url: "http://127.0.0.1:8606/mcp"
  proximity_mcp_url: "http://127.0.0.1:8607/mcp"
  perception_mcp_url: "http://127.0.0.1:8608/mcp"
  safety_mcp_url: "http://127.0.0.1:8609/mcp"

memorizer:
  # If enabled, the advisor will:
  # - ask the MemorizerAgent to decide whether to store user utterances
  # - sometimes ask it to recall relevant memories for user questions
  enabled: true

  # Path to the memorizer agent config.
  # Relative paths are resolved from repo root.
  config_path: "agent/memorizer/config.yaml"

  # Store decisions are made by the memorizer (not the advisor).
  ingest_user_utterances: true

  # Only recall when it seems necessary (advisor uses a light heuristic).
  recall_for_questions: true

  # How many results per tier (short + long) to request.
  recall_top_n: 3

  # Safety timeouts (seconds) so the advisor loop can't stall forever.
  ingest_timeout_seconds: 12.0
  recall_timeout_seconds: 12.0

todo:
  # Local-only todo list with optional LLM-based task planning.
  # The advisor will:
  # - inject todo status into its reasoning prompt (optional)
  # - optionally always mention the next open task out loud
  # - use LLM to decompose multi-step requests into sequential tasks
  # - review plans before execution and after completion
  # - generate autonomous todos in alone mode
  enabled: true

  # Path to the todo agent config.
  # Relative paths are resolved from repo root.
  config_path: "agent/todo/config.yaml"

  # If true, always include the todo status block in the brain prompt.
  include_in_prompt: true

  # If true, the advisor will try to mention the next open task in its response.
  mention_next_in_response: true

  # If true, use LLM-based TaskPlanner for multi-step requests.
  # Example: "fahr 2 sekunden links, dann rechts, dann schau hoch und beschreibe was du siehst"
  # will be decomposed into individual tasks and executed sequentially.
  use_task_planner: true

  # Minimum words to trigger task planning (avoid planning for simple commands).
  task_planner_min_words: 6

  # If true, after creating a plan, ask the LLM if the plan is sufficient
  # or if changes are needed. This enables interactive plan refinement.
  review_after_planning: true

  # If true, after completing all tasks, ask the LLM if the mission is truly
  # complete or if additional tasks should be suggested.
  review_after_completion: true

  # If true, in alone mode (no human interaction), the advisor will generate
  # random/autonomous todos like: look around, drive forward, tell a joke, etc.
  # This makes the robot more "alive" when no one is talking to it.
  autonomous_mode_enabled: true

  # --- Review Settings ---
  # If true, after planning tasks, the LLM will review and suggest changes if needed.
  # This helps ensure the plan is complete and makes sense.
  review_after_planning: true

  # If true, after all tasks are done, the LLM will check if the mission is truly complete
  # or if additional tasks should be suggested.
  review_after_completion: true

  # --- Autonomous Mode Settings ---
  # If true, in alone mode the robot will generate its own random todos instead of
  # just observing and thinking out loud.
  # Examples: "schau nach links", "fahr ein Stück vorwärts", "erzähl einen Witz"
  autonomous_mode_enabled: true

interaction:
  # When sound is detected, we attempt STT.
  # If transcript is empty/too short, we can reprompt once.
  min_transcript_chars: 3
  max_listen_attempts: 2
  reprompt_text: "Bitte sag das noch einmal."

  # Optional: set to true to call Vosk stream mode (only if your stack supports it).
  listen_stream: false

  # Allow humans to interrupt speech.
  # If sound is detected while the robot is speaking, we call the speak service's `stop` tool
  # before starting STT, so the mic doesn't just capture the robot's own voice.
  interrupt_speech_on_sound: true

  # Speech gating / coordination:
  # The speak service plays audio asynchronously (it returns quickly with a PID).
  # These settings prevent the advisor from starting another speech (e.g. in alone mode)
  # while the current one is still playing.

  # If true, after calling `speak` the advisor will poll `status` until speech finishes.
  # During this wait it can still stop speech if sound is detected.
  wait_for_speech_finish: true

  # How often to poll the speak service's `status` while waiting.
  speech_status_poll_interval_seconds: 0.2

  # Safety timeout for waiting.
  speech_max_wait_seconds: 120.0

  # If true, skip alone-mode observe/thought/speak while the robot is currently speaking.
  suppress_alone_mode_while_speaking: true

  # When waiting for speech to finish, the mic will often pick up the robot's own voice.
  # Stopping on generic sound therefore tends to cut speech off.
  # Keep this false unless you have a more robust "human interrupt" signal.
  stop_speech_on_sound_while_waiting: false

  # If sound happened during speech playback, force one interaction attempt shortly
  # after speech ends so the human can be captured even if they spoke over the robot.
  post_speech_interaction_grace_seconds: 1.5

  # If the transcript matches one of these, stop speech immediately.
  # (Useful when someone says "Stop!" while the robot is talking.)
  stop_words:
    - stop
    - stopp
    - halt
    - genug
    - ruhig
    - leise

alone:
  # How often to "think out loud" when quiet.
  think_interval_seconds: 20.0
  observation_question: "Briefly describe what you see in front of the robot."
  max_thought_chars: 240

  # Optional: enable a tiny, safety-guarded exploration move in alone mode.
  # Uses `services/observe` tool `observe_direction` + `services/safety` tool `guarded_drive`.
  # Kept off by default for safety.
  explore_enabled: false

  # Signed speed (-100..100). Positive drives forward.
  explore_speed: 20

  # Safety threshold for obstacle blocking.
  explore_threshold_cm: 35.0

  # Duration for "near" actions (e.g., forward/left/right).
  explore_duration_s: 0.6

  # Duration for "far" actions (e.g., go far forward/go far left/go far right).
  explore_far_duration_s: 1.2

  # If true, speak brief messages when movement is blocked/unavailable.
  explore_speak: false

sound_activity:
  # Sound activity detection.
  # The detector tries:
  # 1) sounddevice (if installed)
  # 2) arecord (ALSA)
  # If neither works, it will behave as "always quiet".
  enabled: true
  # Tune this using the debug protocol's "sound" events.
  # Start around 250-400 for typical USB mics; increase if it triggers too easily.
  threshold_rms: 1200
  # Require N consecutive windows above threshold before entering interaction mode.
  # This reduces false triggers from brief ambient noise spikes.
  active_windows_required: 2
  sample_rate_hz: 16000
  window_seconds: 0.15
  poll_interval_seconds: 0.25

  # Optional: choose a specific ALSA capture device for arecord, e.g.:
  #   arecord_device: "plughw:1,0"
  arecord_device: ""

  # If sound detection is unavailable or errors, fall back to interaction mode
  # (attempt listening) rather than staying in alone mode forever.
  fallback_to_interaction_on_error: true

memory:
  # Approximate token budgeting.
  # We estimate tokens ~ chars/4 (English average).
  max_tokens: 30000
  avg_chars_per_token: 4
  summary_dir: "memory/advisor"
  summary_max_chars: 2500
